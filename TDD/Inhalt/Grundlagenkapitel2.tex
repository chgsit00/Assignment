\chapter{Textmining}
\label{textmining}
Wie bereits in Kapitel \ref{aufbau} angesprochen, werden Datamining und Textmining oft in Relation betrachtet. In der Disziplin Datamining wird das Ziel verfolgt, die Daten mithilfe von Algorithmen, nach Mustern zu durchsuchen, um die, für den entsprechenden Fall, nützlichen Informationen zu finden. Laut Ian H. Witten ist dabei die größte Herausforderung und die aufwändigste Arbeit, die vorhandenen Daten erst in das gewünschte Ausgangsformat zu bringen und anschließend herauszufinden, wonach gesucht wird \cite[vgl.][]{WFH11}. Wo ist Walter? Ist das zu suchende Muster unbekannt, kann Walter kaum gefunden werden. Liegt jedoch die Information vor, dass er immer einen rot-weiß gestreiften Pullover, eine Brille und eine Pudelmütze trägt, kann ihn ein Algorithmus ohne größeren Aufwand ausfindig machen.\\
Zusammengefasst geht es beim Datamining, um Techniken, die strukturelle Muster finden und beschreiben [vgl.][]\cite{WFH11}. Dieser Aufwand wird in Kauf genommen, damit möglichst viele nützliche Informationen aus den vorhandenen Daten \glqq geschürft\grqq werden können.\\
Sollen Textdokumente klassifiziert oder nach Informationen  durchsucht werden, muss auf ein Teilgebiet des Datamining zurückgegriffen werden, dem Textmining. Mit Abbildung \ref{schnittstelle-text-data} wird deutlich, wie diese beiden Disziplinen zusammenhängen.
\begin{figure}[!htb]
	\centering
	\includegraphics[width =0.75\textwidth]{SchnittstelleTextData.jpg}
	\caption[Schnittstellen des Textmining mit den sechs Teilgebieten]{Schnittstellen des Textmining mit den sechs Teilgebieten \cite[vgl.][]{Min+12}}
	\label{schnittstelle-text-data}
\end{figure}
An der Schnittstelle von Datamining und Textmining steht die Klassifikation von Dokumenten (Document Classification). Genau das entspricht der Qualitätsprüfung von Anforderungen.
Der wesentlichste Unterschied zum Textmining ist der, dass beim Datamining, Muster in strukturierten Daten (z.B. Messwerte) gesucht werden. Textdokumente sind hingegen eine unstrukturierte Analysebasis. Lediglich aus der jeweiligen Grammatik der Sprache können Strukturen erkannt werden. Dieser Umstand macht es nötig, die Disziplin der Computerlinguistik hinzuzuziehen, welche sich mit der computergesteuerten Verarbeitung menschlicher Sprache beschäftigt. Der computerlinguistische Teil der Problemstellung der Qualitätsanalyse von Anforderungsdokumenten ist nicht Gegenstand dieser Arbeit, weshalb hier nicht weiter darauf eingegangen wird. Für interessierte Leser wird auf \cite{Car+10} verwiesen.\\
Abbildung \ref{prozess-textmining} zeigt die einzelnen Schritte im Prozess des Textmining. Diese Arbeit bewegt sich im Bereich der Textmining Methoden, sowie der Evaluation der Ergebnisse.
\begin{figure}[!htb]
	\centering
	\includegraphics[width =0.9\textwidth]{textMiningProzessVisio.png}
	\caption[Der Prozess des Textmining]{Der Prozess des Textmining \cite[vgl.][]{HR06}}
	\label{prozess-textmining}
\end{figure}
		
		\section{Document Classification und dessen Algorithmen}
		\label{document-und-algorithmen}
		Sowohl im Datamining als auch im Textmining werden Algorithmen als Werkzeug genutzt. Sie basieren auf mathematischen und statistischen Grundlagen.\\
		Wie bereits erwähnt (Kap. \ref{textmining}) entspricht die Qualitätsprüfung von Anforderungsdokumenten dem Anwendungsgebiet der Document Classification. Hier geht es nicht um Informationsgewinnung, sondern darum, die Dokumente in Kategorien einzuteilen. Es ist zwischen binären und multiplen Klassifikationen zu unterscheiden. Bei der binären Klassifikation werden die Dokumente in zwei verschiedene Kategorien (z.B. Qualitätsprüfung von Anforderungsdokumenten: gut, schlecht) unterteilt. Mehrere Kategorien hingegen, finden sich bei der multiplen Klassifikation. In der vorliegenden Bachelorarbeit wird sich ausschließlich auf binäre Klassifikation der Anforderungen konzentriert. Diese werden in qualitativ minderwertige oder hochwertige Anforderungen klassifiziert. Mathematisch kann die Textklassifikation folgendermaßen definiert werden \cite[vgl.][]{MRS08}:
		
		\begin{minipage}{1\textwidth}
			\fbox{\parbox[c]{0.98\linewidth}{
					Sei $X$ die Menge der zu klassifizierenden Dokumente und $d \in X$ ein Dokument. Sei $C$ die Menge der Kategorien, in die $d$ klassifiziert werden kann und $c \in C$ eine Kategorie.
					Die Klassifikation ist eine Funktion $\phi$, die $d$ einer Klasse $c$ zuordnet:
					\begin{equation}
					\phi := X \rightarrow C
					\end{equation}
				}}
		\end{minipage}\\\\		
		Bei dem \textit{supervised Learning} (beaufsichtigtem Lernen) wird versucht, eine möglichst gute Funktion $\phi$ aus der Menge $D$  abzuleiten.
		
			\begin{minipage}{1\textwidth}
				\fbox{\parbox[c]{0.98\linewidth}{
						Sei $D$ die Menge aller manuell klassifizierten Dokumente.
						\begin{equation}
							D = \{(d,c)|(d,c) \in X \times C\}
						\end{equation}
					}}
				\end{minipage}\\\\		
		Unter dem supervised Learning werden Lerntechniken verstanden, bei denen vordefinierte Kategorien auf Trainingssätze gelegt werden \cite[vgl.][]{Kha+10}. Die Funktion $\phi$ wird automatisiert gesucht. Dabei helfen verschiedene Algorithmen. Im Gegensatz zum supervised Learning, gibt es noch die Lerntechniken des \textit{semi-supervised} und \textit{unsupervised} Learning. Für interessierte Leser wird an dieser Stelle \cite{HS99} \cite{CSZ06} empfohlen. Diese Arbeit beschränkt sich auf das supervised Learning.\\
		Folgende Algorithmen sind laut \cite{Min+12} und \cite{WFH11} am besten für die Document Classification geeignet.
		\begin{itemize}
			\item \textbf{Na\"{i}ve Bayes}\\
			\vspace{-5mm}
			\item \textbf{Singular value decomposition (SVD)}\\
			\vspace{-5mm}
			\item \textbf{Logistic regression}\\
			\vspace{-5mm}
			\item \textbf{C4.5}\\
			\vspace{-5mm}
			\item \textbf{Neural network}\\
			\vspace{-5mm}
			\item \textbf{Support vector machines}\\
			\vspace{-5mm}
			\item \textbf{k-NN}\\
			\vspace{-5mm}
			\item \textbf{RIPPER}\\ 
		\end{itemize}
		Diese Algorithmen stammen aus dem Gebiet des maschinellen Lernens. Ian H. Witten beschreibt hier das Wort \textit{lernen} als Veränderung des Verhaltens durch künstliche Erfahrung auf eine Weise, die erlaubt zukünftig bessere Ergebnisse zu erzielen \cite[vgl.][]{WFH11}.
		
		\section{Ansätze der Algorithmen}
		\label{ansaetze-der-algorithmen}
		Die im Kapitel \ref{document-und-algorithmen} aufgezählten Algorithmen verwenden unterschiedliche Ansätze bei ihrem Vorgehen. Im Datamining werden sie in acht Kategorien eingestuft \cite[vgl.][]{WFH11}:
		\begin{itemize}
			\item \textbf{Ableiten rudimentärer Regeln}\\
			Hier wird auf Grund eines Attributes entschieden, wie die Daten klassifiziert werden sollen. Dieses Vorgehen wird auch 1R (One-Rule) genannt.
			\item \textbf{Statistische Modelle}\\
			Im Gegensatz zu 1R werden bei diesen Modellen alle Attribute berücksichtigt. Es wird grundlegend davon ausgegangen, dass alle Attribute dasselbe Maß an Bedeutung haben und unabhängig voneinander sind.
			\item \textbf{Divide et impera}\footnote{Divide and Conquer, Engl.: Teile und Herrsche}\\
			Bei diesem Vorgehen werden Entscheidungsbäume erstellt. Es wird ein Attribut gewählt, welches als 'Root Node'\footnote{Engl.: Wurzelknoten, repräsentiert die Wurzel eines Entscheidungsbaumes} dient. Für jeden Wert dieses Attributes werden Zweige erstellt, an die weitere Knoten angehängt werden, welche andere Attribute darstellen. Dies wird rekursiv fortgesetzt.
			\item \textbf{Covering\footnote{Engl.: Abdeckung, Bedeckung}-Algorithmen}\\
			Auch hier werden Entscheidungsbäume erstellt. Es wird versucht, alle Daten in einen Bereich zu legen. Gleichzeitig werden alle Daten, die nicht in dieses Muster fallen, \glqq ausgesiebt\grqq. Diese werden mit dem gleichen Verfahren weiterhin ausgemustert.
			\item \textbf{Aufstellung von Verknüpfungsregeln}\\
			Für dieses Vorgehen müssen alle Kombinationen der vorhandenen Attribute erstellt werden. Da die Anzahl der Attribute oftmals sehr hoch ist, ist der Aufwand für die Erstellung meist unzumutbar.
			\item \textbf{Lineare Modelle}\\
			Bei den bisherigen Ansätzen wurden nur nominale Attribute beachtet. Die linearen Modelle beziehen sich ausschließlich auf die numerischen Attribute. Das einfachste Beispiel wäre die Erstellung einer Regressionsgerade.
			\item \textbf{Instanzenbasiertes Lernen}\\
			Hier werden Trainingsdaten mit den zu klassifizierenden Daten verglichen, um den 'Nearest Neighbor'\footnote{Engl.: Nächster Nachbar} zu finden. Dieser wird zur Klasse der nächsten Trainingsdaten gezählt.
			\item \textbf{Clustering}\\
			Das Clustering wird angewandt, wenn für die Daten bisher keine Klassen zur Einteilung vorhanden sind. Dieser Ansatz ist weitaus komplexer als die bisherigen.
		\end{itemize}
		Diese wissenschaftliche Arbeit wird sich nicht mit allen vorgestellten Algorithmen beschäftigen, da es den vorgegebenen Rahmen überschreiten würde. Welcher gewählte Algorithmus zu welchem Ansatz gehört, wird in Kapitel \ref{algorithmen} in der jeweiligen Ausführung erwähnt.
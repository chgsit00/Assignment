\chapter{Aufbau des Versuchs}
\label{daten-und-pipeline}
In diesem Kapitel werden die für die durchgeführten Versuche genutzten Daten vorgestellt (Kap. \ref{verwendete-daten}). Beim Datamining (sowie im Textmining, insbesondere der Textklassifikation) werden solche Versuche auch als Experimente bezeichnet \cite[vgl.][]{WFH11}, weswegen der Begriff des Experiments folgend verwendet wird. Weiterhin soll in diesem Teil der Arbeit, die nötige Vorverarbeitung (Kap. \ref{vorverarbeitung}), die Erstellung der Attribute (Kap. \ref{features}) und die verwendete Pipeline (Kap. \ref{core}) erklärt werden. Zusätzlich wird das genutzte Framework und der gebrauchte Lerner veranschaulicht.\\
Für eine detailliertere Beschreibung der einzelnen Versuchskomponenten wird auf die Arbeit von Herr Zieschang \cite{Zie16} verwiesen.


	\section{Die verwendeten Daten}
	\label{verwendete-daten}
	Wie bereits in Kapitel \ref{aufgabenstellung} beschrieben, sind alle für das Experiment verwendeten Daten aus dem Bereich der Automobilentwicklung. Sie wurden im Rahmen der Bachelorarbeit von der Firma IT-Designers GmbH zur internen Verwendung zur Verfügung gestellt. Somit handelt es sich um reale Anforderungsdokumente aus der Industrie und Praxis. Diese liegen im XML-Format vor. Ein Anforderungsdokument besteht nicht nur aus Anforderungen, sondern hat zusätzlich erklärende Kommentare, Bilder, Überschriften und Kontaktdaten projektbetreuender Personen. Jeder Anforderung ist eine eindeutige ID zugewiesen.\\
	In der vorliegenden Form waren die Anforderungsdokumente nicht für das Experiment nutzbar. Wie diese für die Zwecke des Experiments bearbeitet worden sind, wird im nächsten Abschnitt erläutert.
	
	\section{Das Preprocessing}
	\label{vorverarbeitung}
	Die meisten der zur Verfügung gestellten Daten, die für Experimente und Analyse verwendet werden sollen, liegen in einer Form vor, mit der nicht direkt gearbeitet werden kann. Dies liegt daran, dass die Daten mehr Informationen enthalten als benötigt. Das sind Informationen die für die spezifische Problemstellung irrelevant sind. Darunter fallen Kommentare, Bilder, Überschriften und Kontaktdaten. Solche Daten werden auch \textit{Noisy Data}\footnote{Engl.: verrauschte Daten} genannt und können die Ergebnisse von Experimenten massiv beeinflussen, was die Experimente nicht tragfähig macht. Beispielsweise könnten weniger wichtige Attribute von den Algorithmen als wichtig erachtet werden, da diese häufig in den Noisy Data vorkommen. Werden diese vorher herausgefiltert, wird die Häufigkeit dieses Attributes um einiges geringer. Somit sinkt die Relevanz des Attributes gleichermaßen.\\
	Aus diesem Grund ist das \textit{Preprocessing}\footnote{Engl.: Vorverarbeitung} notwendig. Bei dem Preprocessing werden die Daten auf einen einheitlichen Standard gebracht, welcher für die bevorstehenden Experimente benötigt wird. Die Daten werden gefiltert, damit sie möglichst wenig irrelevante Informationen enthalten. Dabei muss beachtet werden, dass die Ursprungsdaten nicht zu sehr verfälscht werden. Im Fall dieser Arbeit wurden Kommentare, Bilder, Überschriften und Kontaktdaten entfernt. Zumal die Art der Darstellung der Dokumente ebenso zum Preprocessing gehört \cite[vgl.][]{Kha+10}, wurden die gesäuberten Daten als Textdokumente gespeichert, um die weitere Verarbeitung zu vereinfachen. Für das supervised Learning (Kapitel \ref{document-und-algorithmen}) sind bereits klassifizierte Daten erforderlich. Deswegen müssen die Anforderungen zuerst per Hand in die Klassen \textit{qualitativ hochwertig} und \textit{qualitativ minderwertig} zugeordnet werden.\\
	Für die Experimente wird das Framework DKPro TC \cite{Dax+14} verwendet (siehe Kap. \ref{tc}). Das Framework verlangt, dass jede Anforderung in ein eigenes Textdokument geschrieben wird, da DKPro TC dies für die Inputdaten voraussetzt.\\
	
		\subsection{DKPro Core}
		\label{core}
		Da es sich bei den Anforderungen um natürlichsprachigen Text handelt, müssen die Anforderungen zusätzlich mit diversen NLP\footnote{Abk.: Natural Language Processing, engl.: Verarbeitung natürlicher Sprache (Computerlinguistik)}-Werkzeugen vorbereitet werden, um nachfolgend Features (Kap. \ref{features}) darauf anwenden zu können. Gegenfalls muss eine Satzgrenzerkennung oder Tokenisierung durchgeführt werden. Ebenso können verschiedene Parser auf die Anforderungen angewandt werden.\\
		Da Programme, beziehungsweise Werkzeuge im Allgemeinen oftmals inkompatible In- und Outputs haben, ist es schwierig sie hintereinander zu verwenden, wie sie benötigt werden. Hierzu ist eine Pipeline\footnote{Engl.: Rohrleitung} notwendig, die die Formatprobleme zwischen den einzelnen Werkzeugen ausgleicht. DKPro Core\footnote{\url{https://www.ukp.tu-darmstadt.de/software/dkpro-core/}} bietet eine solche Pipeline \cite[vgl.][]{CG14}. Entwickelt wurde sie vom \textit{Ubiquitous Knowledge Processing Lab} der Universität Darmstadt. Diese Pipeline verbindet verschiedene NLP-Werkzeuge miteinander, die nach Bedürfnis eingesetzt werden können.\\
		Mit Hilfe von DKPro Core können nun Features für die Anforderungen erstellt werden. Obwohl der Prozess der Definition von Features ebenfalls zu dem Preprocessing gehört \cite[vgl.][]{Kha+10}, wird dieser anschließend in einem separaten Unterkapitel beschrieben.\\
		
	
	\section{Die Features}
	\label{features}
	Im Gegensatz zu den Daten in Tabelle \ref{wetterdaten-numerisch}, sind die Attribute für Anforderungen nicht vorgegeben oder gar klar. Die Attribute mussten eigens für diese Arbeit erstellt werden. Dies geschieht mit Hilfe sogenannter Features. Meistens entspricht ein Feature einem Attribut. Es kann jedoch vorkommen, dass aus einem Feature mehrere Attribute generiert werden (vgl. N-Gram-Feature in der folgenden Aufzählung). Aufbauend auf den Erkenntnissen aus Kapitel \ref{requirement-engineering} folgt eine Aufzählung der ausgearbeiteten Features mit ihren Erklärungen.
	\begin{itemize}
		\item \textbf{Kleine Schablone}\\
		DKPro stellt einen Parser zur Verfügung der für diese Arbeit genutzt worden ist. Es handelt sich dabei um den PCFG-Parser. Damit ist es möglich einen Satz als Phrasenstrukturbaum darzustellen. Dabei wird der Satz in einzelne Phrasen zerlegt. Anhand dieser Phrasen ist es möglich zu überprüfen ob der Aufbau der Anforderung den Schablonen aus Kapitel \ref{satzstruktur} entspricht. Bei diesem Feature soll auf die Schablone, die in Abbildung \ref{satzschablone} dargestellt wird, geprüft werden.
		\item \textbf{Große Schablone}\\
		Wie beim vorherigem Feature soll hier ein Attribut durch Prüfung des Attributes auf eine vorgegebene Schablone (Abbildung \ref{satzschablone-bedingung}) erstellt werden.
		\item \textbf{Tiefe des Satzes}\\
		Bei der Arbeit mit dem PCFG-Parser ist aufgefallen, dass er im Wesentlichen bei qualitativ schlechten Anforderungen tiefere Bäume aufbaut. Je verschachtelter ein Satz ist, desto tiefer wird der Baum. Wie in Kapitel \ref{qualitätskriterien} und \ref{satzstruktur} diskutiert wurde, ist diese Eigenschaft bei Anforderungen nicht erwünscht. Das erstellte Attribut aus diesem Feature gibt die numerische Tiefe des Satzes bzw. dessen Baumes an.
		\item \textbf{Anzahl der Phrasen}\\
		Ebenso mit Hilfe des Parsers, wurden die Phrasen einer Anforderung gezählt. Viele Phrasen bedeuten im Regelfall lange Sätze. Diese Eigenschaft deutet wiederum auf qualitativ schlechte Merkmale. Das erzeugte Attribut aus diesem Feature gibt die Anzahl der Phrasen einer Anforderung an.
		\item \textbf{Struktur}\\
		Dieses Feature überprüft, ähnlich den Features \textit{Kleine Schablone} und \textit{Große Schablone}, den Aufbau des Satzes. Beispielsweise wird geprüft ob ein Verb am Satzanfang steht. Bei qualitativ hochwertigen Anforderung steht das Verb meist im hinteren Teil des Satzes, da es sich dabei um den Prozess handelt. Am Anfang des Satzes wäre demnach ein Substantiv erwünscht, da dieses das System beschreiben würde.
		\item \textbf{Bad Words}\\
		Dieses Feature erstellt ein Attribut, das die Anzahl der \textit{Bad Words} angibt. Im Rahmen dieser Bachelorarbeit wurden alle Wörter Bad Words genannt, die ein Indiz dafür sind, dass es sich um eine qualitativ schlechte Anforderung handelt. Beispielweise Wörter die eine Negation aufzeigen (nicht, kein). In Kapitel \ref{requirement-engineering} wurde darauf eingegangen.
		\item \textbf{Konjunktionen}\\
		Dieses Feature ist dem Bad Words-Feature sehr ähnlich. Hier werden Konjunktionen in den Anforderungen gezählt. Diese Worte weisen auf Verbindungen zwischen Sätzen hin. In Kapitel \ref{satzstruktur} wurde gezeigt, dass dies bei Anforderungen nicht erwünscht ist.
		\item \textbf{Count Tokens}\\
		Hier wurde die Anzahl der vorkommenden Wörter in einer Anforderung gezählt. Je mehr Wörter eine Anforderung hat, desto länger ist sie. Nach Kapitel \ref{satzstruktur} sollen Anforderungen möglichst kurz gefasst werden.
		\item \textbf{Modalverb}\\
		Das mit am wichtigste an einer Anforderung ist das Modalverb. Dieses gibt die rechtliche Verbindlichkeit an (Kap. \ref{verbindlichkeit}). Existiert kein Modalverb in einer Anforderung, ist diese rechtlich nicht verbindlich. Hier wird ein Attribut erstellt das anzeigt ob ein Modalverb in der Anforderung vorhanden ist. Ist das nicht der Fall, kann direkt von einer qualitativ schlechten Anforderung ausgegangen werden.
		\item \textbf{Anzahl der Kommas}\\
		Hieraus wurde ein Attribut generiert, dass die Anzahl der Kommas in der Anforderung wiedergibt. Der Grund für dieses Attribut ist, dass in qualitativ guten Anforderungen maximal zwei bis drei Kommas vorkommen sollten. Eine höhere Anzahl von Kommas deutet auf einen verschachtelten Satz hin, was bei Anforderungen unerwünscht ist (Kap. \ref{satzstruktur}).
		\item \textbf{Anzahl der Sätze}\\
		Wie aus Kapitel \ref{satzstruktur} hervorgeht, soll pro Anforderung nur ein Satz verwendet werden. Dieses Feature erstellt ein Attribut, dass die Anzahl der Sätze in den Anforderungen anzeigt.
		\item \textbf{Abhängigkeitstokens}\\
		Für dieses Feature wurde ein \textit{Dependenz-Parser} verwendet. Mit einem Dependenz-Parser können Abhängigkeiten der Wörter eines Satzes ermittelt werden. Des Weiteren kann überprüft werden um welche Art von Abhängigkeit es sich dabei handelt.
		\item \textbf{Bedeutung des Modalverbs}\\
		Wie bereits in Kapitel \ref{verbindlichkeit} erwähnt, ist das Modalverb essentiell für eine Anforderung. Es kann jedoch vorkommen, dass ein Modalverb in einem Satz nicht als dieses verwendet wird, sondern als ein Hilfsverb im Satz steht. Als solches stellt das Modalverb keine rechtliche Verbindlichkeit sicher. Somit deutet dieser Fall auf eine qualitativ minderwertige Anforderung, es sei denn, es existiert ein weiteres Modalverb in dem Satz, dass auch wirklich als solches verwendet wird. Mit Hilfe des Dependenz-Parser kann dieser Umstand geprüft werden.
		\item \textbf{N-Gram}\\
		Aus diesem Feature werden $N$-viele Attribute generiert. Mit dem N-Gramm werden in diesem Fall die Anzahl der vorkommenden Wörter in den jeweiligen Klassen gezählt. Die daraus resultierenden Attribute geben die Häufigkeit der $N$ häufigsten Wortes in einer Klasse an.
	\end{itemize}
		
	\section{Das Framework}
	\label{tc}
	Jedes Textmining-Experiment läuft mit einem bestimmten Prozess ab, wie Abbildung \ref{prozess-textmining} zeigt. Vereinfacht, kann der Prozess auch wie folgt dargestellt werden.
		\begin{equation*}
			\text{Einlesen der Daten} \to \text{Preprocessing} \to \text{Trainieren} \to \text{Evaluation}
		\end{equation*}
	DKPro TC\footnote{https://www.ukp.tu-darmstadt.de/software/dkpro-text-classification/} ist ein Framework, das auf Text Classification ausgerichtet ist. Es erstellt überwachte Lernexperimente, die nach obigem Muster aufgebaut werden \cite[vgl.][]{Dax+14}. Ein großer Vorteil liegt hier darin, dass sich das Framework um die Verknüpfung der Textminingschritte kümmert. Es muss nur darauf geachtet werden, dass die Inputdaten in gewünschter Form vorliegen. In dem Fall von DKPro TC muss jede Anforderung in eine Textdatei geschrieben werden. Des Weiteren müssen die benötigten Features und ein Lerner angegeben werden, sowie die Algorithmen, welche vom Lerner bereit gestellt werden.\\
	Im nächsten Abschnitt wird der verwendete Lerner beschrieben.
	
	\section{Der Lerner}
	\label{weka}
	Textklassifikationen können in zwei unterschiedliche Gruppen eingeteilt werden. Die eine ist das sogenannte \textit{Single Label Experiment}, wobei die andere \textit{Multi Label Experiment} genannt wird. Bei der ersteren kann ein Text nur einer Kategorie zugeordnet werden. Bei dem Multi Label Experiment kann er hingegen mehreren Kategorien zugeordnet werden. Ein gutes Beispiel für das Single Label Experiment ist ein Hund. Wird vorausgesetzt der Hund ist reinrassig, kann er entweder Labrador oder Mastiff sein. Beides gleichzeitig ist nicht möglich. Die Artikel in der Online-Enzyklopädie Wikipedia\footnote{\url{https://www.wikipedia.org/}} sind hingegen meist in mehrere Kategorien eingeteilt. Der Artikel über den Vogel Wekaralle beispielsweise, ist in zwei Kategorien eingeteilt: \textit{Endemischer Vogel Neuseelands} und \textit{Rallenvögel}.\\
	Obwohl das verwendete Framework DKProTC drei verschieden Lerner bereitstellt (WEKA\footnote{\url{http://www.cs.waikato.ac.nz/ml/weka/}}, MEKA\footnote{\url{http://meka.sourceforge.net/}}, Mallet\footnote{\url{http://mallet.cs.umass.edu/}}), sind nicht alle für den Rahmen dieser Bachelorarbeit geeignet. Beispielsweise ist MEKA ausschließlich für Multi Label Experimente gedacht. MALLET wiederum bietet eine eingeschränkte Auswahl an Algorithmen. Da es sich bei diesem Projekt um ein Single Label Experiment handelt und WEKA eine breite Auswahl an Algorithmen bietet wurde sich letztendlich für diesen entschieden.\\